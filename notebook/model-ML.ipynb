{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re # for regular expressions\n",
    "import pandas as pd \n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk # for text manipulation\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train  = pd.read_csv('train_E6oV3lV.csv')\n",
    "test = pd.read_csv('test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text PreProcessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      0   \n",
       "5   6      0   \n",
       "6   7      0   \n",
       "7   8      0   \n",
       "8   9      0   \n",
       "9  10      0   \n",
       "\n",
       "                                                                                                                                             tweet  \n",
       "0                                            @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1                       @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                                                              bihday your majesty  \n",
       "3                                                           #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                                                           factsguide: society now    #motivation  \n",
       "5                             [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo    \n",
       "6                                                                        @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦  \n",
       "7  the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl  \n",
       "8                                                          we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦   \n",
       "9                                                                                                @user @user welcome here !  i'm   it's so #gr8 !   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['label'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>@user lets fight against  #love #peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>@user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>how the #altright uses  &amp;amp; insecurity to lure men into #whitesupremacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>@user i'm not interested in a #linguistics that doesn't address #race &amp;amp; . racism is about #power. #raciolinguistics bringsâ¦</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label  \\\n",
       "13    14      1   \n",
       "14    15      1   \n",
       "17    18      1   \n",
       "23    24      1   \n",
       "34    35      1   \n",
       "56    57      1   \n",
       "68    69      1   \n",
       "77    78      1   \n",
       "82    83      1   \n",
       "111  112      1   \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "13                                                          @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot    \n",
       "14                               no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins  \n",
       "17                                                                                                              retweet if you agree!   \n",
       "23                                                                                     @user @user lumpy says i am a . prove it lumpy.  \n",
       "34                            it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia   \n",
       "56                                                                                             @user lets fight against  #love #peace   \n",
       "68                      ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness    \n",
       "77                                             @user hey, white people: you can call people 'white' by @user  #race  #identity #medâ¦  \n",
       "82                                                       how the #altright uses  &amp; insecurity to lure men into #whitesupremacy      \n",
       "111  @user i'm not interested in a #linguistics that doesn't address #race &amp; . racism is about #power. #raciolinguistics bringsâ¦  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['label'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31962, 3), (17197, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi = train.append(test, ignore_index=True)\n",
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Removing Twitter Handles (@user)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                                         tidy_tweet  \n",
       "0                   when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1    thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                               bihday your majesty  \n",
       "3                            #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                            factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Removing Punctuations, Numbers, and Special Characters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction    #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for #lyft credit i can t use cause they don t offer wheelchair vans in pdx     #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model   i love u take with u all the time in ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide  society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo</td>\n",
       "      <td>huge fan fare and big talking before they leave  chaos and pay disputes when they get there  #allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @user @user @user @user dannyâ¦</td>\n",
       "      <td>camping tomorrow        danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "      <td>the next school year is the year for exams      can t think about that      #school #exams   #hate #imagine #actorslife #revolutionschool #girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦</td>\n",
       "      <td>we won    love the land    #allin #cavs #champions #cleveland #clevelandcavaliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr8 !</td>\n",
       "      <td>welcome here    i m   it s so #gr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "5   6    0.0   \n",
       "6   7    0.0   \n",
       "7   8    0.0   \n",
       "8   9    0.0   \n",
       "9  10    0.0   \n",
       "\n",
       "                                                                                                                                             tweet  \\\n",
       "0                                            @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1                       @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                                              bihday your majesty   \n",
       "3                                                           #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                                           factsguide: society now    #motivation   \n",
       "5                             [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo     \n",
       "6                                                                        @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦   \n",
       "7  the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl   \n",
       "8                                                          we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦    \n",
       "9                                                                                                @user @user welcome here !  i'm   it's so #gr8 !    \n",
       "\n",
       "                                                                                                                                        tidy_tweet  \n",
       "0                                                  when a father is dysfunctional and is so selfish he drags his kids into his dysfunction    #run  \n",
       "1                                   thanks for #lyft credit i can t use cause they don t offer wheelchair vans in pdx     #disapointed #getthanked  \n",
       "2                                                                                                                              bihday your majesty  \n",
       "3                                                           #model   i love u take with u all the time in ur                                        \n",
       "4                                                                                                           factsguide  society now    #motivation  \n",
       "5                                   huge fan fare and big talking before they leave  chaos and pay disputes when they get there  #allshowandnogo    \n",
       "6                                                                                                                 camping tomorrow        danny     \n",
       "7  the next school year is the year for exams      can t think about that      #school #exams   #hate #imagine #actorslife #revolutionschool #girl  \n",
       "8                                                          we won    love the land    #allin #cavs #champions #cleveland #clevelandcavaliers        \n",
       "9                                                                                                            welcome here    i m   it s so #gr      "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "combi.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Removing Short Words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take another look at the first few rows of the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when father dysfunctional selfish drags kids into dysfunction #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks #lyft credit cause they offer wheelchair vans #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model love take with time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1    0.0   \n",
       "1   2    0.0   \n",
       "2   3    0.0   \n",
       "3   4    0.0   \n",
       "4   5    0.0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                      tidy_tweet  \n",
       "0             when father dysfunctional selfish drags kids into dysfunction #run  \n",
       "1  thanks #lyft credit cause they offer wheelchair vans #disapointed #getthanked  \n",
       "2                                                            bihday your majesty  \n",
       "3                                                     #model love take with time  \n",
       "4                                                 factsguide society #motivation  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [when, father, dysfunctional, selfish, drags, kids, into, dysfunction, #run]\n",
       "1    [thanks, #lyft, credit, cause, they, offer, wheelchair, vans, #disapointed, #getthanked]\n",
       "2                                                                     [bihday, your, majesty]\n",
       "3                                                            [#model, love, take, with, time]\n",
       "4                                                          [factsguide, society, #motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can normalize the tokenized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "    \n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features from Cleaned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Word2Vec Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6510028, 7536020)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=200, # desired no. of features/independent variables \n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play a bit with our Word2Vec model and see how does it perform. We will specify a word and the model will pull out the most similar words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spaghetti', 0.5658013820648193),\n",
       " ('#avocado', 0.5653101801872253),\n",
       " ('#cellar', 0.5547047853469849),\n",
       " ('cookout', 0.5511012077331543),\n",
       " ('noodl', 0.5489310622215271),\n",
       " ('melani', 0.547335147857666),\n",
       " ('#biall', 0.546588122844696),\n",
       " ('gown', 0.5430378913879395),\n",
       " ('#foodcoma', 0.5413259267807007),\n",
       " ('spinach', 0.5404106378555298)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"dinner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.5460224747657776),\n",
       " ('phoni', 0.5259741544723511),\n",
       " ('unfit', 0.5246831178665161),\n",
       " ('unstabl', 0.5200092792510986),\n",
       " ('melo', 0.5164598226547241),\n",
       " ('potu', 0.515663743019104),\n",
       " ('unfavor', 0.5101866126060486),\n",
       " ('hillari', 0.5076572299003601),\n",
       " ('#delegaterevolt', 0.5051215291023254),\n",
       " ('jibe', 0.5032232999801636)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15007606,  0.14879119, -0.51990169,  0.0058126 ,  0.55208695,\n",
       "        0.25509292,  0.07453623, -0.13314515, -0.0254878 , -0.8908332 ,\n",
       "       -0.02133939,  0.89018846, -0.14146671, -0.34468454, -0.47961965,\n",
       "        0.03066353,  0.35275963, -0.57058471,  1.03962195,  0.12884547,\n",
       "        0.59137058, -0.34608129,  0.92952412,  0.653409  ,  0.28881171,\n",
       "        0.55403763, -0.34932148, -0.73746759, -0.38691986,  0.68097055,\n",
       "        0.13209412, -0.44750923,  0.29219675,  0.63686401, -0.30297968,\n",
       "       -0.03825129,  0.62068158, -0.62892973,  0.15462588,  0.1439736 ,\n",
       "       -0.10439953,  0.02763413, -0.45384923, -0.00450154,  0.20950264,\n",
       "        0.3756882 , -0.23810436,  0.22051652, -0.30991587,  0.41085938,\n",
       "        0.03515792,  0.07115516, -0.41318294,  0.62097359,  0.25477788,\n",
       "       -0.20776244, -0.72082233,  0.13771147, -0.71929592,  0.11429328,\n",
       "        0.2206036 ,  0.24659269, -0.96678174,  0.98984069, -0.02419505,\n",
       "        0.41572711, -0.17525066,  0.37907407, -0.21024323, -0.62315828,\n",
       "        0.39220247,  0.20188518,  0.34157351, -0.53269279,  0.03763888,\n",
       "       -0.29426393,  0.2054625 , -0.43705827,  0.25044039, -0.24679622,\n",
       "        0.67416072, -0.67289978, -0.48204809, -0.97117424, -0.30714279,\n",
       "        0.42861882, -0.06790878, -0.14522843, -0.42536703,  0.47236034,\n",
       "       -0.63712251, -0.96000892,  0.07974151,  0.25426412,  0.1906969 ,\n",
       "        0.48031083,  0.17527717,  0.42015558, -0.87493235,  0.24464223,\n",
       "        1.08253467, -0.17896391, -0.13219987,  0.3883971 ,  0.02994647,\n",
       "        0.3245416 , -0.29796079, -0.49378172,  0.80830252, -0.10045221,\n",
       "        0.38219792,  0.27322048,  0.04144816,  0.2422674 ,  0.64382648,\n",
       "       -0.3711468 ,  0.49309272, -0.60873193, -0.46271139,  0.47515544,\n",
       "        0.09082295, -0.25910863,  0.05061042,  0.2590718 ,  0.00440796,\n",
       "       -0.22002698,  0.37606686,  0.34843382, -0.2715351 , -0.08971476,\n",
       "        0.06571087, -0.18973967, -0.35962474,  0.00556282, -0.16604713,\n",
       "        0.45029792,  0.32906559,  0.30952054,  0.74116272, -1.25179255,\n",
       "        0.31527328,  0.419824  , -0.53012145,  0.33158424,  0.77047271,\n",
       "       -0.54991871,  0.76820081,  0.50645679,  0.49994075,  0.60128528,\n",
       "       -0.00750624,  0.12363218, -0.6370244 ,  0.29522142, -0.29805934,\n",
       "       -0.11819029,  0.18606399, -0.49186444,  0.01151775,  0.12069881,\n",
       "       -0.80829483,  0.13237464,  0.09028237, -0.44283563, -0.20080626,\n",
       "       -0.69564581,  0.53329039,  0.30282548,  0.08716848,  0.53611708,\n",
       "       -0.96495646,  0.0116515 ,  0.35164613, -0.67445683,  0.13109499,\n",
       "       -0.4415434 , -0.12365375, -0.10757735,  0.72350281, -0.15514924,\n",
       "       -0.26206878,  0.38556293,  0.17972343,  0.36686096, -0.25163049,\n",
       "       -0.45159557,  0.123597  ,  0.34224105,  0.20177038,  0.38732901,\n",
       "        0.01403304, -0.02501263,  0.1044465 , -0.38610229,  0.41346085,\n",
       "        0.59413546, -0.06027097, -0.12144137,  0.48135847,  0.08364052], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v['food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v['food']) #The length of the vector is 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['when', 'father', 'dysfunct', 'selfish', 'drag', 'kid', 'into', 'dysfunct', '#run'], tags=['tweet_0']),\n",
       " LabeledSentence(words=['thank', '#lyft', 'credit', 'caus', 'they', 'offer', 'wheelchair', 'van', '#disapoint', '#getthank'], tags=['tweet_1']),\n",
       " LabeledSentence(words=['bihday', 'your', 'majesti'], tags=['tweet_2']),\n",
       " LabeledSentence(words=['#model', 'love', 'take', 'with', 'time'], tags=['tweet_3']),\n",
       " LabeledSentence(words=['factsguid', 'societi', '#motiv'], tags=['tweet_4']),\n",
       " LabeledSentence(words=['huge', 'fare', 'talk', 'befor', 'they', 'leav', 'chao', 'disput', 'when', 'they', 'there', '#allshowandnogo'], tags=['tweet_5'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "100%|██████████| 49159/49159 [00:00<00:00, 2182504.95it/s]\n"
     ]
    }
   ],
   "source": [
    "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model \n",
    "                                  dm_mean=1, # dm = 1 for using mean of the context word vectors\n",
    "                                  size=200, # no. of desired features\n",
    "                                  window=5, # width of the context window\n",
    "                                  negative=7, # if > 0 then negative sampling will be used\n",
    "                                  min_count=5, # Ignores all words with total frequency lower than 2.\n",
    "                                  workers=3, # no. of cores\n",
    "                                  alpha=0.1, # learning rate\n",
    "                                  seed = 23)\n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preparing doc2vec Feature Set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "\n",
    "for i in range(len(combi)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))\n",
    "    \n",
    "docvec_df = pd.DataFrame(docvec_arrays)\n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],  \n",
    "                                                          random_state=42, \n",
    "                                                          test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53078202995008317"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain_bow, ytrain) # training the model\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int) # calculating f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf = tfidf[31962:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54465075154730325"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_w2v = wordvec_df.iloc[:31962,:]\n",
    "test_w2v = wordvec_df.iloc[31962:,:]\n",
    "\n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62222222222222223"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(xtrain_w2v, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_d2v = docvec_df.iloc[:31962,:]\n",
    "test_d2v = docvec_df.iloc[31962:,:]\n",
    "\n",
    "xtrain_d2v = train_d2v.iloc[ytrain.index,:]\n",
    "xvalid_d2v = train_d2v.iloc[yvalid.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36731107205623909"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(xtrain_d2v, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_d2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50837988826815639"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain)\n",
    "\n",
    "prediction = svc.predict_proba(xvalid_bow)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51048313582497717"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = svc.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61381475667189955"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain)\n",
    "\n",
    "prediction = svc.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20273348519362189"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_d2v, ytrain)\n",
    "\n",
    "prediction = svc.predict_proba(xvalid_d2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55292259083728279"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain)\n",
    "\n",
    "prediction = rf.predict(xvalid_bow)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = rf.predict(test_bow)\n",
    "test['label'] = test_pred\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_rf_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56215213358070504"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = rf.predict(xvalid_tfidf)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50752688172043015"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain)\n",
    "\n",
    "prediction = rf.predict(xvalid_w2v)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056737588652482275"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain)\n",
    "\n",
    "prediction = rf.predict(xvalid_d2v)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51306873184898372"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n",
    "prediction = xgb_model.predict(xvalid_bow)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = xgb_model.predict(test_bow)\n",
    "test['label'] = test_pred\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_xgb_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51858913250714966"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = xgb.predict(xvalid_tfidf)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65229110512129374"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = xgb.predict(xvalid_w2v)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_d2v, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34490481522956323"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = xgb.predict(xvalid_d2v)\n",
    "f1_score(yvalid, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain)\n",
    "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid)\n",
    "dtest = xgb.DMatrix(test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters that we are going to tune\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label().astype(np.int)\n",
    "    preds = (preds >= 0.3).astype(np.int)\n",
    "    return [('f1_score', f1_score(labels, preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(6,10) \n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=6, min_child_weight=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tF1 Score 0.6751088000000001 for 64 rounds\n",
      "CV with max_depth=6, min_child_weight=6\n",
      "\tF1 Score 0.6703884 for 55 rounds\n",
      "CV with max_depth=6, min_child_weight=7\n",
      "\tF1 Score 0.6761038 for 57 rounds\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "\tF1 Score 0.6784994 for 51 rounds\n",
      "CV with max_depth=7, min_child_weight=6\n",
      "\tF1 Score 0.6808281999999999 for 47 rounds\n",
      "CV with max_depth=7, min_child_weight=7\n",
      "\tF1 Score 0.6781346000000001 for 115 rounds\n",
      "CV with max_depth=8, min_child_weight=5\n",
      "\tF1 Score 0.6634426 for 36 rounds\n",
      "CV with max_depth=8, min_child_weight=6\n",
      "\tF1 Score 0.6822174000000001 for 71 rounds\n",
      "CV with max_depth=8, min_child_weight=7\n",
      "\tF1 Score 0.6618758 for 32 rounds\n",
      "CV with max_depth=9, min_child_weight=5\n",
      "\tF1 Score 0.6580265999999999 for 32 rounds\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "\tF1 Score 0.673238 for 43 rounds\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "\tF1 Score 0.6738529999999999 for 54 rounds\n",
      "Best params: 8, 6, F1 Score: 0.6822174000000001\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0. # initializing with 0\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        feval= custom_eval,\n",
    "        num_boost_round=200,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Finding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['max_depth'] = 8\n",
    "params['min_child_weight'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(5,10)]\n",
    "    for colsample in [i/10. for i in range(5,10)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['subsample'] = .9\n",
    "params['colsample_bytree'] = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tF1 Score 0.6849056000000001 for 84 rounds\n",
      "CV with eta=0.2\n",
      "\tF1 Score 0.684616 for 93 rounds\n",
      "CV with eta=0.1\n",
      "\tF1 Score 0.6864700000000001 for 211 rounds\n",
      "CV with eta=0.05\n",
      "\tF1 Score 0.6846697999999999 for 200 rounds\n",
      "CV with eta=0.01\n",
      "\tF1 Score 0.1302024 for 0 rounds\n",
      "CV with eta=0.005\n",
      "\tF1 Score 0.1302024 for 0 rounds\n",
      "Best params: 0.1, F1 Score: 0.6864700000000001\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0.\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # Update ETA\n",
    "    params['eta'] = eta\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        feval= custom_eval,\n",
    "        num_boost_round=1000,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=20\n",
    "    )\n",
    "\n",
    "    # Finding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['eta'] = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets tune __gamma__ value using the parameters already tuned above. We’ll check for 5 values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with gamma=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tF1 Score 0.6516993999999999 for 46 rounds\n",
      "CV with gamma=0.1\n",
      "\tF1 Score 0.6425626 for 65 rounds\n",
      "CV with gamma=0.2\n",
      "\tF1 Score 0.6429636 for 64 rounds\n",
      "CV with gamma=0.3\n",
      "\tF1 Score 0.6433358 for 64 rounds\n",
      "CV with gamma=0.4\n",
      "\tF1 Score 0.6502844 for 81 rounds\n",
      "CV with gamma=0.5\n",
      "\tF1 Score 0.6497812 for 69 rounds\n",
      "CV with gamma=0.6\n",
      "\tF1 Score 0.6416596 for 48 rounds\n",
      "CV with gamma=0.7\n",
      "\tF1 Score 0.6511064 for 61 rounds\n",
      "CV with gamma=0.8\n",
      "\tF1 Score 0.648954 for 75 rounds\n",
      "CV with gamma=0.9\n",
      "\tF1 Score 0.6470513999999999 for 64 rounds\n",
      "CV with gamma=1.0\n",
      "\tF1 Score 0.6607518 for 110 rounds\n",
      "CV with gamma=1.1\n",
      "\tF1 Score 0.660221 for 83 rounds\n",
      "CV with gamma=1.2\n",
      "\tF1 Score 0.6627242 for 117 rounds\n",
      "CV with gamma=1.3\n",
      "\tF1 Score 0.6492042 for 63 rounds\n",
      "CV with gamma=1.4\n",
      "\tF1 Score 0.6435412 for 54 rounds\n",
      "Best params: 1.2, F1 Score: 0.6627242\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0.\n",
    "best_params = None\n",
    "for gamma in range(0,15):\n",
    "    print(\"CV with gamma={}\".format(gamma/10.))\n",
    "\n",
    "    # Update ETA\n",
    "    params['gamma'] = gamma/10.\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        feval= custom_eval,\n",
    "        num_boost_round=200,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Finding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = gamma/10.\n",
    "\n",
    "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['gamma'] = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the final list of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample': 0.9,\n",
       " 'colsample_bytree': 0.5,\n",
       " 'eta': 0.1,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 6,\n",
       " 'objective': 'binary:logistic',\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tValidation-error:0.065909\tValidation-f1_score:0.133165\n",
      "Multiple eval metrics have been passed: 'Validation-f1_score' will be used for early stopping.\n",
      "\n",
      "Will train until Validation-f1_score hasn't improved in 10 rounds.\n",
      "[1]\tValidation-error:0.058922\tValidation-f1_score:0.133165\n",
      "[2]\tValidation-error:0.056523\tValidation-f1_score:0.133165\n",
      "[3]\tValidation-error:0.055793\tValidation-f1_score:0.133165\n",
      "[4]\tValidation-error:0.054229\tValidation-f1_score:0.133165\n",
      "[5]\tValidation-error:0.054437\tValidation-f1_score:0.371141\n",
      "[6]\tValidation-error:0.053916\tValidation-f1_score:0.45042\n",
      "[7]\tValidation-error:0.053082\tValidation-f1_score:0.5158\n",
      "[8]\tValidation-error:0.053082\tValidation-f1_score:0.565517\n",
      "[9]\tValidation-error:0.052873\tValidation-f1_score:0.577889\n",
      "[10]\tValidation-error:0.052039\tValidation-f1_score:0.592938\n",
      "[11]\tValidation-error:0.05183\tValidation-f1_score:0.601116\n",
      "[12]\tValidation-error:0.051309\tValidation-f1_score:0.603037\n",
      "[13]\tValidation-error:0.052039\tValidation-f1_score:0.607988\n",
      "[14]\tValidation-error:0.051935\tValidation-f1_score:0.597892\n",
      "[15]\tValidation-error:0.051726\tValidation-f1_score:0.601688\n",
      "[16]\tValidation-error:0.050162\tValidation-f1_score:0.602484\n",
      "[17]\tValidation-error:0.050057\tValidation-f1_score:0.600624\n",
      "[18]\tValidation-error:0.049953\tValidation-f1_score:0.616352\n",
      "[19]\tValidation-error:0.050266\tValidation-f1_score:0.611465\n",
      "[20]\tValidation-error:0.049849\tValidation-f1_score:0.617343\n",
      "[21]\tValidation-error:0.049953\tValidation-f1_score:0.613909\n",
      "[22]\tValidation-error:0.049223\tValidation-f1_score:0.608347\n",
      "[23]\tValidation-error:0.049327\tValidation-f1_score:0.611557\n",
      "[24]\tValidation-error:0.04964\tValidation-f1_score:0.613893\n",
      "[25]\tValidation-error:0.049327\tValidation-f1_score:0.61576\n",
      "[26]\tValidation-error:0.049223\tValidation-f1_score:0.61251\n",
      "[27]\tValidation-error:0.048806\tValidation-f1_score:0.616626\n",
      "[28]\tValidation-error:0.048493\tValidation-f1_score:0.618856\n",
      "[29]\tValidation-error:0.048284\tValidation-f1_score:0.625202\n",
      "[30]\tValidation-error:0.048076\tValidation-f1_score:0.620243\n",
      "[31]\tValidation-error:0.048076\tValidation-f1_score:0.622871\n",
      "[32]\tValidation-error:0.047972\tValidation-f1_score:0.624493\n",
      "[33]\tValidation-error:0.048076\tValidation-f1_score:0.628757\n",
      "[34]\tValidation-error:0.047867\tValidation-f1_score:0.626526\n",
      "[35]\tValidation-error:0.047972\tValidation-f1_score:0.626623\n",
      "[36]\tValidation-error:0.048076\tValidation-f1_score:0.626016\n",
      "[37]\tValidation-error:0.048702\tValidation-f1_score:0.624898\n",
      "[38]\tValidation-error:0.048076\tValidation-f1_score:0.627642\n",
      "[39]\tValidation-error:0.047763\tValidation-f1_score:0.629268\n",
      "[40]\tValidation-error:0.047659\tValidation-f1_score:0.632006\n",
      "[41]\tValidation-error:0.04745\tValidation-f1_score:0.62987\n",
      "[42]\tValidation-error:0.04672\tValidation-f1_score:0.634225\n",
      "[43]\tValidation-error:0.046824\tValidation-f1_score:0.630894\n",
      "[44]\tValidation-error:0.047033\tValidation-f1_score:0.62996\n",
      "[45]\tValidation-error:0.046824\tValidation-f1_score:0.631408\n",
      "[46]\tValidation-error:0.046512\tValidation-f1_score:0.633631\n",
      "[47]\tValidation-error:0.046407\tValidation-f1_score:0.634818\n",
      "[48]\tValidation-error:0.04599\tValidation-f1_score:0.633117\n",
      "[49]\tValidation-error:0.046199\tValidation-f1_score:0.635332\n",
      "[50]\tValidation-error:0.045782\tValidation-f1_score:0.640259\n",
      "[51]\tValidation-error:0.045886\tValidation-f1_score:0.637751\n",
      "[52]\tValidation-error:0.046303\tValidation-f1_score:0.636071\n",
      "[53]\tValidation-error:0.046199\tValidation-f1_score:0.635559\n",
      "[54]\tValidation-error:0.046094\tValidation-f1_score:0.635634\n",
      "[55]\tValidation-error:0.045782\tValidation-f1_score:0.638911\n",
      "[56]\tValidation-error:0.04526\tValidation-f1_score:0.638911\n",
      "[57]\tValidation-error:0.045364\tValidation-f1_score:0.641026\n",
      "[58]\tValidation-error:0.045052\tValidation-f1_score:0.6416\n",
      "[59]\tValidation-error:0.045886\tValidation-f1_score:0.644338\n",
      "[60]\tValidation-error:0.045052\tValidation-f1_score:0.645883\n",
      "[61]\tValidation-error:0.044947\tValidation-f1_score:0.646965\n",
      "[62]\tValidation-error:0.044947\tValidation-f1_score:0.64687\n",
      "[63]\tValidation-error:0.045573\tValidation-f1_score:0.647528\n",
      "[64]\tValidation-error:0.045364\tValidation-f1_score:0.644177\n",
      "[65]\tValidation-error:0.045364\tValidation-f1_score:0.646918\n",
      "[66]\tValidation-error:0.045364\tValidation-f1_score:0.646302\n",
      "[67]\tValidation-error:0.045156\tValidation-f1_score:0.650602\n",
      "[68]\tValidation-error:0.045052\tValidation-f1_score:0.654429\n",
      "[69]\tValidation-error:0.04526\tValidation-f1_score:0.65655\n",
      "[70]\tValidation-error:0.045052\tValidation-f1_score:0.656\n",
      "[71]\tValidation-error:0.044947\tValidation-f1_score:0.653815\n",
      "[72]\tValidation-error:0.045052\tValidation-f1_score:0.651685\n",
      "[73]\tValidation-error:0.04526\tValidation-f1_score:0.650641\n",
      "[74]\tValidation-error:0.044843\tValidation-f1_score:0.653846\n",
      "[75]\tValidation-error:0.045156\tValidation-f1_score:0.652209\n",
      "[76]\tValidation-error:0.045156\tValidation-f1_score:0.652209\n",
      "[77]\tValidation-error:0.044947\tValidation-f1_score:0.653784\n",
      "[78]\tValidation-error:0.045573\tValidation-f1_score:0.655367\n",
      "[79]\tValidation-error:0.045364\tValidation-f1_score:0.653226\n",
      "Stopping. Best iteration:\n",
      "[69]\tValidation-error:0.04526\tValidation-f1_score:0.65655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    feval= custom_eval,\n",
    "    num_boost_round= 1000,\n",
    "    maximize=True,\n",
    "    evals=[(dvalid, \"Validation\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = xgb_model.predict(dtest)\n",
    "test['label'] = (test_pred >= 0.3).astype(np.int)\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_xgb_w2v_06062018.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score: 0.793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
