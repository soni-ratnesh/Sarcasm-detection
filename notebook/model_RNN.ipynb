{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm Detection RNN\n",
    "In this notebook, we'll implement a recurrent neural network that performs sarcasm detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train_data_clean.csv')\n",
    "reviews = list(data['tweet'].apply(str))\n",
    "labels = list(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' NAME I urge all charity working for to help translate khanacademy into local languages & amp make it available offline', ' NAME Wonder how your members of Congress voted on key legislation  Check it out by putting in your zip code htt…', ' The Electric Company Party Game Lost on Prankster Planet iOSApp/iOSAp by PBSKIDS HYPERLINK', ' Ethics Where do morals come from  via Philip Gorski NAME HYPERLINK writing politics', ' Star Rover HD Stargazing and Night Sky Watching Education  HYPERLINK ipad Reference', ' Lessons versus Learning by NAME on NAME HYPERLINK administration bullying schoolviolence', \" Here's a great channel to watch to help you getyourhealthyon HYPERLINK\", \" familyplanning access to economic resources are all women's rights but still a dream in many countries HYPERLINK\", ' NAME Scholarships for young leaders who aspire to make a difference in the world HYPERLINK scholarship edu…', ' Mastering a new language can completely rewire your brain says studyHYPERLINK HYPERLINK']\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:10])\n",
    "print()\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word for sent in reviews for word in sent.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51692"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary \n",
    "counts = Counter(corpus)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = []\n",
    "for review in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  7,\n",
       "  1457,\n",
       "  28,\n",
       "  724,\n",
       "  237,\n",
       "  9,\n",
       "  4,\n",
       "  123,\n",
       "  1458,\n",
       "  2662,\n",
       "  160,\n",
       "  725,\n",
       "  486,\n",
       "  17,\n",
       "  24,\n",
       "  97,\n",
       "  15,\n",
       "  524,\n",
       "  1905]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ints[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 31\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(review_lens.keys(), review_lens.values())\n",
    "plt.title(\"Tweet length distribution\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1 1906   45   31 1907    6 1178 1908   13  525 4677  293   15   33\n",
      "   34 1179    8   31 4678 2663]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 20\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(3004, 20) \n",
      "Validation set: \t(374, 20) \n",
      "Test set: \t\t(374, 20)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.80\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx+1], features[split_idx-1:]\n",
    "train_y, remaining_y = labels[:split_idx+1], labels[split_idx-1:]\n",
    "\n",
    "print(len(remaining_x))\n",
    "test_idx = int((len(remaining_x)-4)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx+4:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx+4:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(np.asarray(train_x)), torch.from_numpy(np.asarray(train_y)))\n",
    "valid_data = TensorDataset(torch.from_numpy(np.asarray(val_x)), torch.from_numpy(np.asarray(val_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(np.asarray(test_x)), torch.from_numpy(np.asarray(test_y)))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([32, 20])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,    0,    1, 1542,   11,  353, 2818,   21,    8,   20,\n",
      "         1543, 1994,   28, 2819,  228,    2, 2820,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    1, 1600,  618, 1285,\n",
      "         1623,   56,  243,  215,  141,    2,  324,  243],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1068, 1869,\n",
      "            4,   20,  619,   79, 8469, 2530, 8470,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    1,  111,    1,  836, 6186, 1310,\n",
      "            6,  468, 2184, 6187,    2,   61,    1,    2],\n",
      "        [   0,    0,  270,    4,   58,   18,  262,    6,    3, 2035,  198,  176,\n",
      "            8, 1239,  227,  363,    9,  835,    2,  760],\n",
      "        [ 130,   23,    4,  476, 1822,  821,   14,    8,  208,  677, 7878,   23,\n",
      "            5,  464,    4,   66,  161,  112,   21, 7879],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    1, 2056, 2927,  206, 1584,\n",
      "           67,  869,   56, 2928, 1031, 2929,  227,    2],\n",
      "        [   0,    0, 9334,   42,  147,   90, 9335,  874,   25,   15,  959,   53,\n",
      "           44, 4426, 1796,   75,   44,  148, 4426,  551],\n",
      "        [   0,    0,    0,    0,    0,    0, 1273,    4,   19,    3, 2997, 2087,\n",
      "           95,    8,    3, 2998,  157,    2, 2999,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    1, 7103,   46, 2390,  676,  269],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  385,  156,   94,   13,  267],\n",
      "        [   0,    0,    0,    0,   63,   10, 1319,    4, 5807,  133, 1222,    9,\n",
      "         1043,  105, 3306, 2074,    8, 5808,   61,    1],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 6748, 6749, 1913,\n",
      "         6750,  392, 1730, 6751, 1098, 6752, 2064,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1626,    5, 6528,\n",
      "         1222,  672, 2330, 3560,    2,  983, 6529, 6530],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,  414, 2467, 1232,   53,\n",
      "           14,   10,  309, 7632,    4,  967, 3604,    2],\n",
      "        [   0,    0,    0, 1652, 1186, 2484,  165,  153,    8,    3,  275,   10,\n",
      "          269,    5,   88,  239,    9,   20, 9718, 9719],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
      "         6008, 6009,   18,    3, 6010, 6011, 6012,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         2628,  774,  413,   12,  197,   51, 4392,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
      "           12, 1822,   32, 1125, 1121, 4495,   11, 9554],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 3723, 2251,   33, 7458, 3798, 3799,  178],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    1,  124,   67,  238,  782,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 2003, 4021,   46,   20,  459,\n",
      "         2268,   63, 8186, 8187,    3,  188, 2557,    2],\n",
      "        [   1,    7,    5, 9564,  221, 9565, 1398,   34,   16,   68,  513,  185,\n",
      "            5, 1158, 2608,   30, 1148,   42,   13,  667],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  169, 1427,\n",
      "           10,    3, 2393,  311,  114, 1146,   14,   98],\n",
      "        [ 640, 1908, 4061,  135,    8,   52,    5, 2522,   85,  373,  538,    4,\n",
      "            3, 4061, 7965, 7966, 1840,  208,   63,   10],\n",
      "        [   0,    0,    0,    0,    0,    0,    1,    1,  276,  822,   10,    5,\n",
      "         1137, 4001, 2496, 7767, 1419, 1137, 4002,    2],\n",
      "        [   0,    0,    0,    0,    0,    1,    1,    1,  298,  148, 9665,  101,\n",
      "           19, 2565,    4,  568,    9, 1371,  415, 9666],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1222, 1057, 5625, 5626,  554,  150,  457,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 1483, 1091, 1498,  547, 6745,\n",
      "         6746, 1587,  152, 6747,    2,  139,  110, 1948],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   27,\n",
      "          547,   72,  863,    4,  531,   11, 1019,    2],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
      "          414,   35,  579,   97,  241, 4309, 4310,    2],\n",
      "        [   0,    0,    0,    0,    0,  198,  176,  363,    4,  344,  777,  528,\n",
      "         5509, 5510, 3158,    2, 5511, 3158,   67,  109]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([32])\n",
      "Sample label: \n",
      " tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(11138, 512)\n",
      "  (lstm): LSTM(512, 100, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 512\n",
    "hidden_dim = 100\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 10... Loss: 0.660176... Val Loss: 0.707615\n",
      "Epoch: 1/2... Step: 20... Loss: 0.573454... Val Loss: 0.558197\n",
      "Epoch: 1/2... Step: 30... Loss: 0.490775... Val Loss: 0.391767\n",
      "Epoch: 1/2... Step: 40... Loss: 0.322353... Val Loss: 0.318318\n",
      "Epoch: 1/2... Step: 50... Loss: 0.408423... Val Loss: 0.382278\n",
      "Epoch: 1/2... Step: 60... Loss: 0.652151... Val Loss: 0.372609\n",
      "Epoch: 1/2... Step: 70... Loss: 0.392112... Val Loss: 0.196500\n",
      "Epoch: 1/2... Step: 80... Loss: 0.234036... Val Loss: 0.347960\n",
      "Epoch: 1/2... Step: 90... Loss: 0.304958... Val Loss: 0.205577\n",
      "Epoch: 2/2... Step: 100... Loss: 0.132183... Val Loss: 0.164994\n",
      "Epoch: 2/2... Step: 110... Loss: 0.192994... Val Loss: 0.161079\n",
      "Epoch: 2/2... Step: 120... Loss: 0.462370... Val Loss: 0.244294\n",
      "Epoch: 2/2... Step: 130... Loss: 0.369860... Val Loss: 0.225495\n",
      "Epoch: 2/2... Step: 140... Loss: 0.069687... Val Loss: 0.194486\n",
      "Epoch: 2/2... Step: 150... Loss: 0.037168... Val Loss: 0.154717\n",
      "Epoch: 2/2... Step: 160... Loss: 0.155006... Val Loss: 0.246366\n",
      "Epoch: 2/2... Step: 170... Loss: 0.118346... Val Loss: 0.241877\n",
      "Epoch: 2/2... Step: 180... Loss: 0.129468... Val Loss: 0.205764\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 2 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 10\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for ii, (inputs, labels) in enumerate(train_loader):\n",
    "        if 3004//batch_size > ii:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for jj, (inputs, labels) in enumerate(valid_loader):\n",
    "                    \n",
    "                    if 374//batch_size > jj:\n",
    "                        # Creating new variables for the hidden state, otherwise\n",
    "                        # we'd backprop through the entire training history\n",
    "                        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                        if(train_on_gpu):\n",
    "                            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        #                 print(inputs.shape)\n",
    "        #                 print(labels.shape)\n",
    "\n",
    "                        output, val_h = net(inputs, val_h)\n",
    "                        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.222\n",
      "Test accuracy: 0.864\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for jj, (inputs, labels) in enumerate(test_loader):\n",
    "    if 374//batch_size > jj:\n",
    "    \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # get predicted outputs\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_to_int.dict', 'wb+') as file:\n",
    "    dill.dump(vocab_to_int, file, dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
